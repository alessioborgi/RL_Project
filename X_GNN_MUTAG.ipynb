{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2WbgJ3vqr0TE"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alessioborgi/RL_Project/blob/main/X_GNN_MUTAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# X-GNN: Model-Explanations of GNNs using RL\n",
        "\n",
        "### *Alessio Borgi*\n",
        "### *Francesco Danese*"
      ],
      "metadata": {
        "id": "JUUvLA8lri-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0: INTALLING & IMPORTING LIBRARIES"
      ],
      "metadata": {
        "id": "2WbgJ3vqr0TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric networkx matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE7aTsSLrdMk",
        "outputId": "40ebc3e5-a602-4bc3-c394-783bf3208c2b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import networkx as nx\n",
        "from torch.nn import Linear\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import plotly.graph_objects as go\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.utils import to_networkx, to_dense_adj\n",
        "from torch_geometric.utils import add_self_loops, remove_self_loops, degree"
      ],
      "metadata": {
        "id": "H6kL0ndKrzz3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensures that dataset splitting, model initialization, and training are deterministic.\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)  # Fix seed for PyTorch (CPU).\n",
        "    torch.cuda.manual_seed(seed)  # Fix seed for PyTorch (GPU).\n",
        "    torch.cuda.manual_seed_all(seed)  # Fix seed for all GPUs.\n",
        "    np.random.seed(seed)  # Fix seed for NumPy.\n",
        "    random.seed(seed)  # Fix seed for Python's random module.\n",
        "    torch.backends.cudnn.deterministic = True  # Ensure deterministic GPU behavior.\n",
        "    torch.backends.cudnn.benchmark = False  # Disable cuDNN auto-tuning to enforce determinism.\n",
        "\n",
        "set_seed(42)  # Set seed to ensure reproducibility across runs."
      ],
      "metadata": {
        "id": "FV3Oe3Ogr6Gi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1: DATASET EXPLORATION"
      ],
      "metadata": {
        "id": "tUwpo2PYr85a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MUTAG Dataset.\n",
        "dataset = TUDataset(root='data/TUDataset', name='MUTAG')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoK2Qv1wr_G8",
        "outputId": "a59fe9d3-14a8-4af5-b77f-e20b64a44163"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists to collect dataset-wide statistics.\n",
        "num_nodes = []\n",
        "num_edges = []\n",
        "labels = []\n",
        "\n",
        "# Gather data about all graphs.\n",
        "for data in dataset:\n",
        "    num_nodes.append(data.num_nodes)\n",
        "    num_edges.append(data.num_edges)\n",
        "    labels.append(data.y.item())\n",
        "\n",
        "# Create a summary DataFrame.\n",
        "df = pd.DataFrame({\n",
        "    \"Graph ID\": range(len(dataset)),\n",
        "    \"Num Nodes\": num_nodes,\n",
        "    \"Num Edges\": num_edges,\n",
        "    \"Label\": labels\n",
        "})\n",
        "\n",
        "# Dataset Statistics.\n",
        "print()\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f\"Number of Graphs: {len(dataset)}\")\n",
        "print(f\"Number of Classes: {dataset.num_classes}\")\n",
        "print(f\"Average Nodes per Graph: {sum(num_nodes)/len(num_nodes):.2f}\")\n",
        "print(f\"Average Edges per Graph: {sum(num_edges)/len(num_edges):.2f}\")\n",
        "print(\"Class Distribution:\")\n",
        "print(df['Label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjM6AWy2sBux",
        "outputId": "54828975-c28e-4394-b5ca-b4d3a28d281a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset: MUTAG(188):\n",
            "====================\n",
            "Number of Graphs: 188\n",
            "Number of Classes: 2\n",
            "Average Nodes per Graph: 17.93\n",
            "Average Edges per Graph: 39.59\n",
            "Class Distribution:\n",
            "Label\n",
            "1    125\n",
            "0     63\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot class distribution using Plotly.\n",
        "class_counts = df['Label'].value_counts().reset_index()\n",
        "class_counts.columns = ['Label', 'Count']\n",
        "\n",
        "fig = px.bar(\n",
        "    class_counts,\n",
        "    x='Label', y='Count',\n",
        "    labels={\"Label\": \"Class Label\", \"Count\": \"Count\"},\n",
        "    title=\"Class Distribution in MUTAG Dataset\"\n",
        ")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "fKGtzVSjsDoF",
        "outputId": "2b6d31d3-d2f6-453a-fa0e-f7d8df766b57"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"385c4fdf-0f22-483f-8652-6f32446e5f97\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"385c4fdf-0f22-483f-8652-6f32446e5f97\")) {                    Plotly.newPlot(                        \"385c4fdf-0f22-483f-8652-6f32446e5f97\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"Class Label=%{x}\\u003cbr\\u003eCount=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[1,0],\"xaxis\":\"x\",\"y\":[125,63],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Class Label\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Count\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Class Distribution in MUTAG Dataset\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('385c4fdf-0f22-483f-8652-6f32446e5f97');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the first graph of the Dataset.\n",
        "single_graph = dataset[0]\n",
        "print()\n",
        "print(single_graph)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph.\n",
        "print(f'Number of nodes: {single_graph.num_nodes}')\n",
        "print(f'Number of edges: {single_graph.num_edges}')\n",
        "print(f'Average node degree: {single_graph.num_edges / single_graph.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {single_graph.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {single_graph.has_self_loops()}')\n",
        "print(f'Is undirected: {single_graph.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tmgyk5Lhszvv",
        "outputId": "1ac05b61-aae4-4b45-f990-1bba1139646f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n",
            "=============================================================\n",
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Average node degree: 2.24\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import plotly.graph_objs as go\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you already have the graph data loaded as `single_graph`\n",
        "# Convert the first graph in the dataset to NetworkX.\n",
        "G = to_networkx(single_graph, to_undirected=True)\n",
        "\n",
        "# Extract the node feature matrix\n",
        "node_features = single_graph.x  # Shape: [num_nodes, num_features]\n",
        "\n",
        "# Convert one-hot encoded node features to atom types (indices)\n",
        "node_colors = torch.argmax(node_features, dim=1).numpy()\n",
        "\n",
        "# Define a colormap for different atom types\n",
        "cmap = plt.get_cmap(\"tab10\")  # You can choose any matplotlib colormap\n",
        "unique_atom_types = np.unique(node_colors)\n",
        "colors = {atom_type: cmap(i / len(unique_atom_types)) for i, atom_type in enumerate(unique_atom_types)}\n",
        "\n",
        "# Generate 2D layout\n",
        "pos_2d = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Prepare edge trace for 2D visualization\n",
        "edge_x_2d = []\n",
        "edge_y_2d = []\n",
        "for edge in G.edges():\n",
        "    x0, y0 = pos_2d[edge[0]]\n",
        "    x1, y1 = pos_2d[edge[1]]\n",
        "    edge_x_2d.extend([x0, x1, None])\n",
        "    edge_y_2d.extend([y0, y1, None])\n",
        "\n",
        "edge_trace_2d = go.Scatter(\n",
        "    x=edge_x_2d, y=edge_y_2d,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines'\n",
        ")\n",
        "\n",
        "# Prepare node trace for 2D visualization\n",
        "node_x_2d = []\n",
        "node_y_2d = []\n",
        "node_labels = []\n",
        "node_colors_plotly = []\n",
        "\n",
        "for node in G.nodes():\n",
        "    x, y = pos_2d[node]\n",
        "    node_x_2d.append(x)\n",
        "    node_y_2d.append(y)\n",
        "    atom_type_list = ['C', 'N', 'O', 'F', 'I', 'Cl', 'Br']\n",
        "    atom_type = atom_type_list[node_colors[node]]\n",
        "    node_labels.append(f\"{atom_type}\")\n",
        "    rgba_color = colors[node_colors[node]]\n",
        "    plotly_color = f\"rgba({rgba_color[0] * 255}, {rgba_color[1] * 255}, {rgba_color[2] * 255}, {rgba_color[3]})\"\n",
        "    node_colors_plotly.append(plotly_color)\n",
        "\n",
        "node_trace_2d = go.Scatter(\n",
        "    x=node_x_2d, y=node_y_2d,\n",
        "    mode='markers+text',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        size=10,\n",
        "        color=node_colors_plotly,\n",
        "    ),\n",
        "    text=node_labels,\n",
        "    textposition=\"top center\"\n",
        ")\n",
        "\n",
        "# 2D Visualization\n",
        "fig_2d = go.Figure(data=[edge_trace_2d, node_trace_2d],\n",
        "                   layout=go.Layout(\n",
        "                       title=\"2D Visualization of Graph 0\",\n",
        "                       showlegend=False,\n",
        "                       hovermode='closest',\n",
        "                       margin=dict(b=0, l=0, r=0, t=40),\n",
        "                       xaxis=dict(showgrid=False, zeroline=False),\n",
        "                       yaxis=dict(showgrid=False, zeroline=False)\n",
        "                   ))\n",
        "fig_2d.show()\n",
        "\n",
        "# Analyze graph properties\n",
        "print(f\"Number of nodes: {single_graph.num_nodes}\")\n",
        "print(f\"Number of edges: {single_graph.num_edges}\")\n",
        "print(f\"Is graph directed? {single_graph.is_directed()}\")\n",
        "print(f\"Graph label: {single_graph.y}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "kf8xM0wOsFi7",
        "outputId": "7a91b622-e3c1-4617-f452-7a1514005541"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"d6fd7f42-8ec2-4faf-a3b1-b048413b3e70\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d6fd7f42-8ec2-4faf-a3b1-b048413b3e70\")) {                    Plotly.newPlot(                        \"d6fd7f42-8ec2-4faf-a3b1-b048413b3e70\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"#888\",\"width\":0.5},\"mode\":\"lines\",\"x\":[-0.3727182492900634,-0.5510840196249834,null,-0.3727182492900634,-0.2299887901282408,null,-0.5510840196249834,-0.5438804985577245,null,-0.5438804985577245,-0.3766813157901222,null,-0.3766813157901222,-0.20885686146700344,null,-0.3766813157901222,-0.2665553478854011,null,-0.20885686146700344,-0.2299887901282408,null,-0.20885686146700344,-0.016589072446311765,null,-0.016589072446311765,0.04449844969654987,null,0.04449844969654987,-0.004549900306905611,null,-0.004549900306905611,-0.2665553478854011,null,-0.004549900306905611,0.17349067894459028,null,-0.2665553478854011,-0.20909489834523434,null,-0.20909489834523434,0.006577112790116703,null,0.006577112790116703,0.30487213864738016,null,0.30487213864738016,0.17349067894459028,null,0.30487213864738016,0.6260641640669481,null,0.6260641640669481,0.7958900025590367,null,0.6260641640669481,0.8286064071373688,null],\"y\":[1.0,0.8570191093083964,null,1.0,0.828528558137815,null,0.8570191093083964,0.5802363441080655,null,0.5802363441080655,0.30682370852924457,null,0.30682370852924457,0.5429957606160726,null,0.30682370852924457,-0.0705188995613755,null,0.5429957606160726,0.828528558137815,null,0.5429957606160726,0.3753641564577354,null,0.3753641564577354,0.12135433599050012,null,0.12135433599050012,-0.1484296524723047,null,-0.1484296524723047,-0.0705188995613755,null,-0.1484296524723047,-0.40582669457225157,null,-0.0705188995613755,-0.3993931571933171,null,-0.3993931571933171,-0.6191922903997025,null,-0.6191922903997025,-0.6515404053515196,null,-0.6515404053515196,-0.40582669457225157,null,-0.6515404053515196,-0.7588467657998171,null,-0.7588467657998171,-0.9358164791562336,null,-0.7588467657998171,-0.6227576286413062,null],\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(31.0, 119.0, 180.0, 1.0)\",\"rgba(214.0, 39.0, 40.0, 1.0)\",\"rgba(227.0, 119.0, 194.0, 1.0)\",\"rgba(227.0, 119.0, 194.0, 1.0)\"],\"size\":10},\"mode\":\"markers+text\",\"text\":[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"N\",\"O\",\"O\"],\"textposition\":\"top center\",\"x\":[-0.3727182492900634,-0.5510840196249834,-0.5438804985577245,-0.3766813157901222,-0.20885686146700344,-0.2299887901282408,-0.016589072446311765,0.04449844969654987,-0.004549900306905611,-0.2665553478854011,-0.20909489834523434,0.006577112790116703,0.30487213864738016,0.17349067894459028,0.6260641640669481,0.7958900025590367,0.8286064071373688],\"y\":[1.0,0.8570191093083964,0.5802363441080655,0.30682370852924457,0.5429957606160726,0.828528558137815,0.3753641564577354,0.12135433599050012,-0.1484296524723047,-0.0705188995613755,-0.3993931571933171,-0.6191922903997025,-0.6515404053515196,-0.40582669457225157,-0.7588467657998171,-0.9358164791562336,-0.6227576286413062],\"type\":\"scatter\"}],                        {\"hovermode\":\"closest\",\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":40},\"showlegend\":false,\"title\":{\"text\":\"2D Visualization of Graph 0\"},\"xaxis\":{\"showgrid\":false,\"zeroline\":false},\"yaxis\":{\"showgrid\":false,\"zeroline\":false},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d6fd7f42-8ec2-4faf-a3b1-b048413b3e70');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 17\n",
            "Number of edges: 38\n",
            "Is graph directed? False\n",
            "Graph label: tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import plotly.graph_objs as go\n",
        "import networkx as nx\n",
        "from torch_geometric.utils import to_networkx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "single_graph = dataset[4]\n",
        "# Assuming you already have the graph data loaded as `single_graph`\n",
        "# Convert the first graph in the dataset to NetworkX.\n",
        "G = to_networkx(single_graph, to_undirected=True)\n",
        "\n",
        "# Extract the node feature matrix\n",
        "node_features = single_graph.x  # Shape: [num_nodes, num_features]\n",
        "\n",
        "# Convert one-hot encoded node features to atom types (indices)\n",
        "node_colors = torch.argmax(node_features, dim=1).numpy()\n",
        "\n",
        "# Define atom labels and fixed colormap\n",
        "atom_labels = ['C', 'N', 'O', 'F', 'I', 'Cl', 'Br']\n",
        "fixed_colors = {\n",
        "    'C': 'rgba(0, 255, 0, 1)',  # Green\n",
        "    'N': 'rgba(255, 0, 0, 1)',  # Red\n",
        "    'O': 'rgba(0, 0, 255, 1)',  # Blue\n",
        "    'F': 'rgba(255, 255, 0, 1)',  # Yellow\n",
        "    'I': 'rgba(128, 0, 128, 1)',  # Purple\n",
        "    'Cl': 'rgba(255, 165, 0, 1)',  # Orange\n",
        "    'Br': 'rgba(75, 0, 130, 1)'   # Indigo\n",
        "}\n",
        "\n",
        "# Generate 3D layout\n",
        "pos_3d = nx.spring_layout(G, dim=3, seed=42)\n",
        "\n",
        "# Extract node labels based on atom types\n",
        "node_labels = []\n",
        "node_colors_plotly = []\n",
        "for node in G.nodes():\n",
        "    atom_type = atom_labels[node_colors[node]]\n",
        "    node_labels.append(atom_type)\n",
        "    plotly_color = fixed_colors[atom_type]\n",
        "    node_colors_plotly.append(plotly_color)\n",
        "\n",
        "# Prepare edge trace for 3D visualization\n",
        "edge_x_3d = []\n",
        "edge_y_3d = []\n",
        "edge_z_3d = []\n",
        "for edge in G.edges():\n",
        "    x0, y0, z0 = pos_3d[edge[0]]\n",
        "    x1, y1, z1 = pos_3d[edge[1]]\n",
        "    edge_x_3d.extend([x0, x1, None])\n",
        "    edge_y_3d.extend([y0, y1, None])\n",
        "    edge_z_3d.extend([z0, z1, None])\n",
        "\n",
        "edge_trace_3d = go.Scatter3d(\n",
        "    x=edge_x_3d, y=edge_y_3d, z=edge_z_3d,\n",
        "    line=dict(width=0.5, color='#888'),\n",
        "    hoverinfo='none',\n",
        "    mode='lines'\n",
        ")\n",
        "\n",
        "# Prepare node trace for 3D visualization\n",
        "node_x_3d = []\n",
        "node_y_3d = []\n",
        "node_z_3d = []\n",
        "for node in G.nodes():\n",
        "    x, y, z = pos_3d[node]\n",
        "    node_x_3d.append(x)\n",
        "    node_y_3d.append(y)\n",
        "    node_z_3d.append(z)\n",
        "\n",
        "node_trace_3d = go.Scatter3d(\n",
        "    x=node_x_3d, y=node_y_3d, z=node_z_3d,\n",
        "    mode='markers+text',\n",
        "    hoverinfo='text',\n",
        "    marker=dict(\n",
        "        size=10,\n",
        "        color=node_colors_plotly,\n",
        "    ),\n",
        "    text=node_labels,\n",
        "    textposition=\"top center\"\n",
        ")\n",
        "\n",
        "# 3D Visualization\n",
        "fig_3d = go.Figure(data=[edge_trace_3d, node_trace_3d],\n",
        "                   layout=go.Layout(\n",
        "                       title=\"3D Visualization of Graph 0\",\n",
        "                       showlegend=False,\n",
        "                       margin=dict(b=0, l=0, r=0, t=40),\n",
        "                       scene=dict(\n",
        "                           xaxis=dict(showticklabels=False),\n",
        "                           yaxis=dict(showticklabels=False),\n",
        "                           zaxis=dict(showticklabels=False)\n",
        "                       )\n",
        "                   ))\n",
        "fig_3d.show()\n",
        "\n",
        "# Analyze graph properties\n",
        "print(f\"Number of nodes: {single_graph.num_nodes}\")\n",
        "print(f\"Number of edges: {single_graph.num_edges}\")\n",
        "print(f\"Is graph directed? {single_graph.is_directed()}\")\n",
        "print(f\"Graph label: {single_graph.y}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        },
        "id": "8dRhlu6ZsP6R",
        "outputId": "75348fe3-bf29-4f42-e7c3-1dfae93fa3c7"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1cc3bd2d-7f5e-4fca-8afd-9d01f9ed8f2f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1cc3bd2d-7f5e-4fca-8afd-9d01f9ed8f2f\")) {                    Plotly.newPlot(                        \"1cc3bd2d-7f5e-4fca-8afd-9d01f9ed8f2f\",                        [{\"hoverinfo\":\"none\",\"line\":{\"color\":\"#888\",\"width\":0.5},\"mode\":\"lines\",\"x\":[0.02458546069025762,-0.2497724533846732,null,0.02458546069025762,0.3506783398707835,null,-0.2497724533846732,-0.33870899413810185,null,-0.33870899413810185,-0.007160738692036449,null,-0.33870899413810185,-0.6893228619804218,null,-0.007160738692036449,0.2481327865347945,null,-0.007160738692036449,0.018497996295593084,null,0.2481327865347945,0.3506783398707835,null,0.3506783398707835,0.5873156864975905,null,0.018497996295593084,-0.08369490102825228,null,0.018497996295593084,0.13944967933446725,null],\"y\":[-0.13351455396521839,-0.050148520252982914,null,-0.13351455396521839,-0.1888365195744066,null,-0.050148520252982914,0.0059437570218711195,null,0.0059437570218711195,0.048923917773303786,null,0.0059437570218711195,-0.010695959720479414,null,0.048923917773303786,-0.0775316434784458,null,0.048923917773303786,0.183619246357252,null,-0.0775316434784458,-0.1888365195744066,null,-0.1888365195744066,-0.3092693102440526,null,0.183619246357252,0.40499888411238844,null,0.183619246357252,0.12651070197077038,null],\"z\":[0.6640739399357102,0.45847939511230384,null,0.6640739399357102,0.5594011291999086,null,0.45847939511230384,0.0871366963812924,null,0.0871366963812924,-0.1966109723140789,null,0.0871366963812924,0.07226906111168803,null,-0.1966109723140789,0.1586094094460366,null,-0.1966109723140789,-0.6772928364646472,null,0.1586094094460366,0.5594011291999086,null,0.5594011291999086,0.8034406719279515,null,-0.6772928364646472,-0.9295064943361648,null,-0.6772928364646472,-1.0,null],\"type\":\"scatter3d\"},{\"hoverinfo\":\"text\",\"marker\":{\"color\":[\"rgba(0, 255, 0, 1)\",\"rgba(0, 255, 0, 1)\",\"rgba(0, 255, 0, 1)\",\"rgba(0, 255, 0, 1)\",\"rgba(0, 255, 0, 1)\",\"rgba(0, 255, 0, 1)\",\"rgba(255, 255, 0, 1)\",\"rgba(255, 0, 0, 1)\",\"rgba(0, 0, 255, 1)\",\"rgba(0, 0, 255, 1)\",\"rgba(255, 255, 0, 1)\"],\"size\":10},\"mode\":\"markers+text\",\"text\":[\"C\",\"C\",\"C\",\"C\",\"C\",\"C\",\"F\",\"N\",\"O\",\"O\",\"F\"],\"textposition\":\"top center\",\"x\":[0.02458546069025762,-0.2497724533846732,-0.33870899413810185,-0.007160738692036449,0.2481327865347945,0.3506783398707835,0.5873156864975905,0.018497996295593084,-0.08369490102825228,0.13944967933446725,-0.6893228619804218],\"y\":[-0.13351455396521839,-0.050148520252982914,0.0059437570218711195,0.048923917773303786,-0.0775316434784458,-0.1888365195744066,-0.3092693102440526,0.183619246357252,0.40499888411238844,0.12651070197077038,-0.010695959720479414],\"z\":[0.6640739399357102,0.45847939511230384,0.0871366963812924,-0.1966109723140789,0.1586094094460366,0.5594011291999086,0.8034406719279515,-0.6772928364646472,-0.9295064943361648,-1.0,0.07226906111168803],\"type\":\"scatter3d\"}],                        {\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":40},\"scene\":{\"xaxis\":{\"showticklabels\":false},\"yaxis\":{\"showticklabels\":false},\"zaxis\":{\"showticklabels\":false}},\"showlegend\":false,\"title\":{\"text\":\"3D Visualization of Graph 0\"},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1cc3bd2d-7f5e-4fca-8afd-9d01f9ed8f2f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 11\n",
            "Number of edges: 22\n",
            "Is graph directed? False\n",
            "Graph label: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for d in dataset:\n",
        "  if d.x[:, 4].sum().item() > 0:\n",
        "    print(i)\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "1ojHYP6z7XqZ",
        "outputId": "08968d7b-1cd6-4079-bbd8-20120fae7597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:150]\n",
        "test_dataset = dataset[150:]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj6g8d1YsXOO",
        "outputId": "e76bd9c9-92e1-4c34-9ff6-5bd265032632"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 150\n",
            "Number of test graphs: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PARTE DEL CINESE\n"
      ],
      "metadata": {
        "id": "DsjhJdjdpibq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title chinese blabla\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "\n",
        "\n",
        "# MUTAG数据集特征，188个图，总共3371个结点，7442条边，为无向图\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "\n",
        "def load_split_MUTAG_data(path=\"/content/datas/\", dataset=\"MUTAG_\", split_train=0.7, split_val=0.15):\n",
        "    \"\"\"Load MUTAG data \"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    # 加载图的标签\n",
        "    graph_labels = np.genfromtxt(\"{}{}graph_labels.txt\".format(path, dataset),\n",
        "                           dtype=np.dtype(int))\n",
        "    graph_labels = encode_onehot(graph_labels)  # (188, 2)\n",
        "    graph_labels = torch.LongTensor(np.where(graph_labels)[1]) # (188, 1)\n",
        "\n",
        "\n",
        "    # 图结点的索引号\n",
        "    graph_idx = np.genfromtxt(\"{}{}graph_indicator.txt\".format(path, dataset),\n",
        "                              dtype=np.dtype(int))\n",
        "\n",
        "    graph_idx = np.array(graph_idx, dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(graph_idx)} # key, value表示第key个图的起始结点索引号为value\n",
        "    length = len(idx_map.keys()) # 总共有多少个图\n",
        "    num_nodes = [idx_map[n] - idx_map[n - 1] if n - 1 > 1 else idx_map[n] for n in range(1, length + 1)] # 一个长度188的list，表示没个图有多少个结点\n",
        "    max_num_nodes = max(num_nodes) # 最大的一个图有多少个结点\n",
        "    features_list = []\n",
        "    adj_list = []\n",
        "    prev = 0\n",
        "\n",
        "    # 结点的标签\n",
        "    nodeidx_features = np.genfromtxt(\"{}{}node_labels.txt\".format(path, dataset), delimiter=\",\",\n",
        "                                     dtype=np.dtype(int))\n",
        "    node_features = np.zeros((nodeidx_features.shape[0], max(nodeidx_features) + 1))\n",
        "    node_features[np.arange(nodeidx_features.shape[0]), nodeidx_features] = 1\n",
        "\n",
        "    # 边信息\n",
        "    edges_unordered = np.genfromtxt(\"{}{}A.txt\".format(path, dataset), delimiter=\",\",\n",
        "                                    dtype=np.int32)\n",
        "\n",
        "    # 边的标签\n",
        "    edges_label = np.genfromtxt(\"{}{}edge_labels.txt\".format(path, dataset), delimiter=\",\",\n",
        "                                dtype=np.int32)  # shape = (7442,)\n",
        "\n",
        "    # 生成邻接矩阵A，该邻接矩阵包括了数据集中所有的边\n",
        "    adj = sp.coo_matrix((edges_label, (edges_unordered[:, 0] - 1, edges_unordered[:, 1] - 1)))\n",
        "\n",
        "    # 论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    node_features = normalize(node_features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0])) # 对应公式A~=A+IN\n",
        "    adj = adj.todense()\n",
        "\n",
        "    for n in range(1, length + 1):\n",
        "        # entry为第n个图的特征矩阵X\n",
        "        entry = np.zeros((max_num_nodes, max(nodeidx_features) + 1))\n",
        "        entry[:idx_map[n] - prev] = node_features[prev:idx_map[n]]\n",
        "        entry = torch.FloatTensor(entry)\n",
        "        features_list.append(entry.tolist())\n",
        "\n",
        "        # entry为第n个图的邻接矩阵A\n",
        "        entry = np.zeros((max_num_nodes, max_num_nodes))\n",
        "        entry[:idx_map[n] - prev, :idx_map[n] - prev] = adj[prev:idx_map[n], prev:idx_map[n]]\n",
        "        entry = torch.FloatTensor(entry)\n",
        "        adj_list.append(entry.tolist())\n",
        "\n",
        "        prev = idx_map[n] # prev为下个图起始结点的索引号\n",
        "\n",
        "    num_total = max(graph_idx)\n",
        "    num_train = int(split_train * num_total)\n",
        "    num_val = int((split_train + split_val) * num_total)\n",
        "\n",
        "    if (num_train == num_val or num_val == num_total):\n",
        "        return\n",
        "\n",
        "    features_list = torch.FloatTensor(features_list)\n",
        "    adj_list = torch.FloatTensor(adj_list)\n",
        "\n",
        "    idx_train = range(num_train)\n",
        "    idx_val = range(num_train, num_val)\n",
        "    idx_test = range(num_val, num_total)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    # 返回值一次为 188个图的邻接矩阵列表  188个图的特征矩阵列表  188个图的label， 每个图的起始结点索引号， 训练集索引号，\n",
        "    # 验证集索引号， 测试集索引号\n",
        "    return adj_list, features_list, graph_labels, idx_map, idx_train, idx_val, idx_test"
      ],
      "metadata": {
        "id": "OMFErq4cpnKd",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GNN MODEL"
      ],
      "metadata": {
        "id": "ztZVpElEp1NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model to Explain (GCN)\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    paper: Semi-Supervised Classification with Graph Convolutional Networks\n",
        "    \"\"\"\n",
        "    # The parameters of the model include weight and bias\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # Weight matrix for the GCN layer\n",
        "        self.bias = Parameter(torch.FloatTensor(out_features))  # Bias term for the GCN layer\n",
        "        self.reset_parameters()  # Initialize parameters\n",
        "\n",
        "    # Weight initialization\n",
        "    def reset_parameters(self):\n",
        "        # Initialize weights and biases using uniform distribution based on the size of the output features\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    # Representation function, similar to __str__\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "    # Compute A~ X W(0), where A~ is the normalized adjacency matrix and X is the input feature matrix\n",
        "    def forward(self, input, adj):\n",
        "        # input.shape = [num_nodes, features] = X\n",
        "        # adj.shape = [num_nodes, num_nodes] = A~\n",
        "        # torch.mm(a, b) performs matrix multiplication of a and b, torch.mul(a, b) performs element-wise multiplication (a and b must have the same dimensions)\n",
        "        support = torch.mm(input, self.weight)  # Matrix multiplication of input features with weights, shape = [max_node, out_features]\n",
        "        output = torch.spmm(adj, support)  # Multiply normalized adjacency matrix with the support matrix, shape = [max_node, out_features]\n",
        "        return output + self.bias  # Add bias term, shape = [max_node, out_features]\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    # nfeat: number of input features, nclass: number of classes for classification, dropout: dropout rate\n",
        "    def __init__(self, nfeat, nclass, dropout):\n",
        "        \"\"\" As per paper \"\"\"\n",
        "        \"\"\" 3 layers of GCNs with output dimensions equal to 32, 48, 64 respectively and average all node features \"\"\"\n",
        "        \"\"\" Final classifier with 2 fully connected layers and hidden dimension set to 32 \"\"\"\n",
        "        \"\"\" Activation function - ReLu (Mutag) \"\"\"\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define three GCN layers with increasing output dimensions\n",
        "        self.gc1 = GraphConvolution(nfeat, 32)  # First GCN layer (input features -> 32)\n",
        "        self.gc2 = GraphConvolution(32, 48)  # Second GCN layer (32 -> 48)\n",
        "        self.gc3 = GraphConvolution(48, 64)  # Third GCN layer (48 -> 64)\n",
        "\n",
        "        # Define two fully connected (linear) layers for classification\n",
        "        self.fc1 = nn.Linear(64, 32)  # First fully connected layer (64 -> 32)\n",
        "        self.fc2 = nn.Linear(32, nclass)  # Second fully connected layer (32 -> nclass)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # x.shape = [max_node, features]\n",
        "        # adj.shape = [max_node, max_node]\n",
        "\n",
        "        # First GCN layer with ReLU activation and dropout\n",
        "        x = F.relu(self.gc1(x, adj))  # x.shape = [num_nodes, 32]\n",
        "        x = F.dropout(x, self.dropout, training=self.training)  # Apply dropout\n",
        "\n",
        "        # Second GCN layer with ReLU activation and dropout\n",
        "        x = F.relu(self.gc2(x, adj))  # x.shape = [num_nodes, 48]\n",
        "        x = F.dropout(x, self.dropout, training=self.training)  # Apply dropout\n",
        "\n",
        "        # Third GCN layer with ReLU activation\n",
        "        x = F.relu(self.gc3(x, adj))  # x.shape = [num_nodes, 64]\n",
        "\n",
        "        # Aggregate all node features by taking the mean of all nodes\n",
        "        y = torch.mean(x, 0)  # y.shape = [64], aggregate using mean over all nodes\n",
        "\n",
        "        # Apply fully connected layers for final classification\n",
        "        y = F.relu(self.fc1(y))  # First fully connected layer with ReLU activation, y.shape = [32]\n",
        "        y = F.dropout(y, self.dropout, training=self.training)  # Apply dropout\n",
        "        y = F.softmax(self.fc2(y), dim=0)  # Second fully connected layer with softmax activation, y.shape = [nclass]\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Random input features of 29 nodes, each with 7 features\n",
        "    input = torch.rand(29, 7)\n",
        "    # Random adjacency matrix of size 29x29\n",
        "    adj = torch.rand(29, 29)\n",
        "\n",
        "    # Initialize the GCN model with 7 input features, 2 output classes, and a dropout rate of 0.1\n",
        "    model = GCN(nfeat=7,  # nfeat = 7\n",
        "                nclass=2,  # nclass = 2\n",
        "                dropout=0.1)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    output = model(input, adj)\n",
        "    print(output.size())  # Output size should be [nclass], which is [2] in this case\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGQ-uM_kp2qX",
        "outputId": "0842d486-4374-4f68-c2ef-807f1b6f2804",
        "cellView": "form"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training the GCN\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_path = 'model/gcn_first.pth'\n",
        "\n",
        "epochs = 1000\n",
        "seed = 200\n",
        "lr = 0.001\n",
        "dropout = 0.1\n",
        "weight_decay = 5e-4\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "class EarlyStopping():\n",
        "    def __init__(self, patience=10, min_loss=0.5, hit_min_before_stopping=False):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.hit_min_before_stopping = hit_min_before_stopping\n",
        "        if hit_min_before_stopping:\n",
        "            self.min_loss = min_loss\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = loss\n",
        "        elif loss > self.best_loss:\n",
        "            self.counter += 1\n",
        "            if self.counter > self.patience:\n",
        "                if self.hit_min_before_stopping == True and loss > self.min_loss:\n",
        "                    print(\"Cannot hit mean loss, will continue\")\n",
        "                    self.counter -= self.patience\n",
        "                else:\n",
        "                    self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = loss\n",
        "            counter = 0\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # adj_list: [188, 29, 29]\n",
        "    # features_list: [188, 29, 7]\n",
        "    # graph_labels: [188]\n",
        "    adj_list, features_list, graph_labels, idx_map, idx_train, idx_val, idx_test = load_split_MUTAG_data()\n",
        "    idx_train = torch.cat([idx_train, idx_val, idx_test])\n",
        "\n",
        "    model = GCN(nfeat=features_list[0].shape[1], # nfeat = 7\n",
        "                nclass=graph_labels.max().item() + 1, # nclass = 2\n",
        "                dropout=dropout)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    model.cuda()\n",
        "    features_list = features_list.cuda()\n",
        "    adj_list = adj_list.cuda()\n",
        "    graph_labels = graph_labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "    # 训练模型\n",
        "    early_stopping = EarlyStopping(10, hit_min_before_stopping=True)\n",
        "    t_total = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # # Split\n",
        "        outputs = []\n",
        "        for i in idx_train:\n",
        "            output = model(features_list[i], adj_list[i])\n",
        "            output = output.unsqueeze(0)\n",
        "            outputs.append(output)\n",
        "        output = torch.cat(outputs, dim=0)\n",
        "\n",
        "\n",
        "        loss_train = F.cross_entropy(output, graph_labels[idx_train])\n",
        "        acc_train = accuracy(output, graph_labels[idx_train])\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        outputs = []\n",
        "        for i in idx_val:\n",
        "            output = model(features_list[i], adj_list[i])\n",
        "            output = output.unsqueeze(0)\n",
        "            outputs.append(output)\n",
        "        output = torch.cat(outputs, dim=0)\n",
        "        loss_val = F.cross_entropy(output, graph_labels[idx_val])\n",
        "        acc_val = accuracy(output, graph_labels[idx_val])\n",
        "\n",
        "        print('Epoch: {:04d}'.format(epoch + 1),\n",
        "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "              'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "              'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "              'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "        print(loss_val)\n",
        "        early_stopping(loss_val)\n",
        "        if early_stopping.early_stop == True:\n",
        "            break\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9591j-ct0L2",
        "outputId": "315d96b8-e84e-48ea-c9e1-d81ad2cc4ab8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MUTAG_ dataset...\n",
            "Epoch: 0001 loss_train: 0.7084 acc_train: 0.3351 loss_val: 0.6937 acc_val: 0.5000 time: 0.2765s\n",
            "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0002 loss_train: 0.7074 acc_train: 0.3351 loss_val: 0.6933 acc_val: 0.5000 time: 0.2643s\n",
            "tensor(0.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0003 loss_train: 0.7053 acc_train: 0.3351 loss_val: 0.6930 acc_val: 0.5000 time: 0.2761s\n",
            "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0004 loss_train: 0.7025 acc_train: 0.3351 loss_val: 0.6928 acc_val: 0.5000 time: 0.2552s\n",
            "tensor(0.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0005 loss_train: 0.7015 acc_train: 0.3351 loss_val: 0.6925 acc_val: 0.5000 time: 0.2628s\n",
            "tensor(0.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0006 loss_train: 0.7004 acc_train: 0.3351 loss_val: 0.6923 acc_val: 0.5000 time: 0.3963s\n",
            "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0007 loss_train: 0.6984 acc_train: 0.3351 loss_val: 0.6921 acc_val: 0.5000 time: 0.3405s\n",
            "tensor(0.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0008 loss_train: 0.6969 acc_train: 0.3351 loss_val: 0.6919 acc_val: 0.5000 time: 0.3809s\n",
            "tensor(0.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0009 loss_train: 0.6958 acc_train: 0.3457 loss_val: 0.6918 acc_val: 0.5000 time: 0.3922s\n",
            "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0010 loss_train: 0.6940 acc_train: 0.3989 loss_val: 0.6917 acc_val: 0.5000 time: 0.4060s\n",
            "tensor(0.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0011 loss_train: 0.6926 acc_train: 0.5638 loss_val: 0.6916 acc_val: 0.7143 time: 0.4418s\n",
            "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0012 loss_train: 0.6915 acc_train: 0.6702 loss_val: 0.6916 acc_val: 0.6071 time: 0.3081s\n",
            "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0013 loss_train: 0.6904 acc_train: 0.6223 loss_val: 0.6916 acc_val: 0.5000 time: 0.2527s\n",
            "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0014 loss_train: 0.6893 acc_train: 0.6489 loss_val: 0.6916 acc_val: 0.5000 time: 0.2709s\n",
            "tensor(0.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0015 loss_train: 0.6875 acc_train: 0.6809 loss_val: 0.6915 acc_val: 0.5000 time: 0.2677s\n",
            "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0016 loss_train: 0.6867 acc_train: 0.6436 loss_val: 0.6915 acc_val: 0.5000 time: 0.2580s\n",
            "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0017 loss_train: 0.6870 acc_train: 0.6383 loss_val: 0.6914 acc_val: 0.5000 time: 0.2659s\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0018 loss_train: 0.6845 acc_train: 0.6755 loss_val: 0.6914 acc_val: 0.5000 time: 0.2879s\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0019 loss_train: 0.6831 acc_train: 0.6702 loss_val: 0.6914 acc_val: 0.5000 time: 0.2703s\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0020 loss_train: 0.6822 acc_train: 0.6649 loss_val: 0.6913 acc_val: 0.5000 time: 0.2922s\n",
            "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0021 loss_train: 0.6819 acc_train: 0.6596 loss_val: 0.6913 acc_val: 0.5000 time: 0.2712s\n",
            "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0022 loss_train: 0.6804 acc_train: 0.6649 loss_val: 0.6912 acc_val: 0.5000 time: 0.2716s\n",
            "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0023 loss_train: 0.6793 acc_train: 0.6649 loss_val: 0.6912 acc_val: 0.5000 time: 0.2522s\n",
            "tensor(0.6912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0024 loss_train: 0.6779 acc_train: 0.6649 loss_val: 0.6911 acc_val: 0.5000 time: 0.2573s\n",
            "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0025 loss_train: 0.6766 acc_train: 0.6649 loss_val: 0.6911 acc_val: 0.5000 time: 0.2704s\n",
            "tensor(0.6911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0026 loss_train: 0.6757 acc_train: 0.6649 loss_val: 0.6910 acc_val: 0.5000 time: 0.2697s\n",
            "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0027 loss_train: 0.6740 acc_train: 0.6649 loss_val: 0.6910 acc_val: 0.5000 time: 0.2603s\n",
            "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0028 loss_train: 0.6721 acc_train: 0.6649 loss_val: 0.6909 acc_val: 0.5000 time: 0.2742s\n",
            "tensor(0.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0029 loss_train: 0.6734 acc_train: 0.6649 loss_val: 0.6907 acc_val: 0.5000 time: 0.2859s\n",
            "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0030 loss_train: 0.6709 acc_train: 0.6649 loss_val: 0.6906 acc_val: 0.5000 time: 0.2582s\n",
            "tensor(0.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0031 loss_train: 0.6678 acc_train: 0.6649 loss_val: 0.6905 acc_val: 0.5000 time: 0.2600s\n",
            "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0032 loss_train: 0.6658 acc_train: 0.6649 loss_val: 0.6905 acc_val: 0.5000 time: 0.2616s\n",
            "tensor(0.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0033 loss_train: 0.6655 acc_train: 0.6649 loss_val: 0.6904 acc_val: 0.5000 time: 0.2750s\n",
            "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0034 loss_train: 0.6651 acc_train: 0.6649 loss_val: 0.6903 acc_val: 0.5000 time: 0.2591s\n",
            "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0035 loss_train: 0.6608 acc_train: 0.6649 loss_val: 0.6902 acc_val: 0.5000 time: 0.2604s\n",
            "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0036 loss_train: 0.6584 acc_train: 0.6649 loss_val: 0.6901 acc_val: 0.5000 time: 0.2785s\n",
            "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0037 loss_train: 0.6578 acc_train: 0.6649 loss_val: 0.6901 acc_val: 0.5000 time: 0.2677s\n",
            "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0038 loss_train: 0.6560 acc_train: 0.6649 loss_val: 0.6900 acc_val: 0.5000 time: 0.2609s\n",
            "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0039 loss_train: 0.6542 acc_train: 0.6649 loss_val: 0.6900 acc_val: 0.5000 time: 0.2668s\n",
            "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0040 loss_train: 0.6513 acc_train: 0.6649 loss_val: 0.6900 acc_val: 0.5000 time: 0.2826s\n",
            "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0041 loss_train: 0.6495 acc_train: 0.6649 loss_val: 0.6900 acc_val: 0.5000 time: 0.2586s\n",
            "tensor(0.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0042 loss_train: 0.6476 acc_train: 0.6649 loss_val: 0.6901 acc_val: 0.5000 time: 0.2597s\n",
            "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0043 loss_train: 0.6431 acc_train: 0.6649 loss_val: 0.6901 acc_val: 0.5000 time: 0.2573s\n",
            "tensor(0.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0044 loss_train: 0.6428 acc_train: 0.6649 loss_val: 0.6902 acc_val: 0.5000 time: 0.2866s\n",
            "tensor(0.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0045 loss_train: 0.6359 acc_train: 0.6649 loss_val: 0.6904 acc_val: 0.5000 time: 0.2588s\n",
            "tensor(0.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0046 loss_train: 0.6335 acc_train: 0.6649 loss_val: 0.6907 acc_val: 0.5000 time: 0.2619s\n",
            "tensor(0.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0047 loss_train: 0.6379 acc_train: 0.6649 loss_val: 0.6910 acc_val: 0.5000 time: 0.2625s\n",
            "tensor(0.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0048 loss_train: 0.6305 acc_train: 0.6649 loss_val: 0.6915 acc_val: 0.5000 time: 0.2709s\n",
            "tensor(0.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0049 loss_train: 0.6279 acc_train: 0.6649 loss_val: 0.6922 acc_val: 0.5000 time: 0.3788s\n",
            "tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Cannot hit mean loss, will continue\n",
            "Epoch: 0050 loss_train: 0.6256 acc_train: 0.6649 loss_val: 0.6930 acc_val: 0.5000 time: 0.3457s\n",
            "tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0051 loss_train: 0.6258 acc_train: 0.6649 loss_val: 0.6939 acc_val: 0.5000 time: 0.3501s\n",
            "tensor(0.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0052 loss_train: 0.6222 acc_train: 0.6649 loss_val: 0.6951 acc_val: 0.5000 time: 0.3429s\n",
            "tensor(0.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0053 loss_train: 0.6212 acc_train: 0.6649 loss_val: 0.6964 acc_val: 0.5000 time: 0.3982s\n",
            "tensor(0.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0054 loss_train: 0.6180 acc_train: 0.6649 loss_val: 0.6979 acc_val: 0.5000 time: 0.3995s\n",
            "tensor(0.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0055 loss_train: 0.6135 acc_train: 0.6649 loss_val: 0.6996 acc_val: 0.5000 time: 0.3986s\n",
            "tensor(0.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0056 loss_train: 0.6113 acc_train: 0.6649 loss_val: 0.7014 acc_val: 0.5000 time: 0.3148s\n",
            "tensor(0.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0057 loss_train: 0.6121 acc_train: 0.6649 loss_val: 0.7034 acc_val: 0.5000 time: 0.2552s\n",
            "tensor(0.7034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0058 loss_train: 0.6103 acc_train: 0.6649 loss_val: 0.7055 acc_val: 0.5000 time: 0.2639s\n",
            "tensor(0.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0059 loss_train: 0.6140 acc_train: 0.6649 loss_val: 0.7076 acc_val: 0.5000 time: 0.2688s\n",
            "tensor(0.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Cannot hit mean loss, will continue\n",
            "Epoch: 0060 loss_train: 0.6096 acc_train: 0.6649 loss_val: 0.7097 acc_val: 0.5000 time: 0.2752s\n",
            "tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0061 loss_train: 0.6103 acc_train: 0.6649 loss_val: 0.7118 acc_val: 0.5000 time: 0.2533s\n",
            "tensor(0.7118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0062 loss_train: 0.6091 acc_train: 0.6649 loss_val: 0.7138 acc_val: 0.5000 time: 0.2743s\n",
            "tensor(0.7138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0063 loss_train: 0.6113 acc_train: 0.6649 loss_val: 0.7156 acc_val: 0.5000 time: 0.2680s\n",
            "tensor(0.7156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0064 loss_train: 0.6080 acc_train: 0.6649 loss_val: 0.7171 acc_val: 0.5000 time: 0.2714s\n",
            "tensor(0.7171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0065 loss_train: 0.6112 acc_train: 0.6649 loss_val: 0.7184 acc_val: 0.5000 time: 0.2603s\n",
            "tensor(0.7184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0066 loss_train: 0.6105 acc_train: 0.6649 loss_val: 0.7194 acc_val: 0.5000 time: 0.2581s\n",
            "tensor(0.7194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0067 loss_train: 0.6105 acc_train: 0.6649 loss_val: 0.7201 acc_val: 0.5000 time: 0.2664s\n",
            "tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0068 loss_train: 0.6137 acc_train: 0.6649 loss_val: 0.7205 acc_val: 0.5000 time: 0.2693s\n",
            "tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0069 loss_train: 0.6062 acc_train: 0.6649 loss_val: 0.7206 acc_val: 0.5000 time: 0.2639s\n",
            "tensor(0.7206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Cannot hit mean loss, will continue\n",
            "Epoch: 0070 loss_train: 0.6086 acc_train: 0.6649 loss_val: 0.7205 acc_val: 0.5000 time: 0.2648s\n",
            "tensor(0.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0071 loss_train: 0.6075 acc_train: 0.6649 loss_val: 0.7201 acc_val: 0.5000 time: 0.2620s\n",
            "tensor(0.7201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0072 loss_train: 0.6109 acc_train: 0.6649 loss_val: 0.7195 acc_val: 0.5000 time: 0.2730s\n",
            "tensor(0.7195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0073 loss_train: 0.6074 acc_train: 0.6649 loss_val: 0.7187 acc_val: 0.5000 time: 0.2610s\n",
            "tensor(0.7187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0074 loss_train: 0.6107 acc_train: 0.6649 loss_val: 0.7177 acc_val: 0.5000 time: 0.2589s\n",
            "tensor(0.7177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0075 loss_train: 0.6092 acc_train: 0.6649 loss_val: 0.7165 acc_val: 0.5000 time: 0.2788s\n",
            "tensor(0.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0076 loss_train: 0.6087 acc_train: 0.6649 loss_val: 0.7153 acc_val: 0.5000 time: 0.2585s\n",
            "tensor(0.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0077 loss_train: 0.6057 acc_train: 0.6649 loss_val: 0.7139 acc_val: 0.5000 time: 0.2596s\n",
            "tensor(0.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0078 loss_train: 0.6109 acc_train: 0.6649 loss_val: 0.7125 acc_val: 0.5000 time: 0.2685s\n",
            "tensor(0.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0079 loss_train: 0.6092 acc_train: 0.6649 loss_val: 0.7111 acc_val: 0.5000 time: 0.2731s\n",
            "tensor(0.7111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Cannot hit mean loss, will continue\n",
            "Epoch: 0080 loss_train: 0.6030 acc_train: 0.6649 loss_val: 0.7097 acc_val: 0.5000 time: 0.2611s\n",
            "tensor(0.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0081 loss_train: 0.6069 acc_train: 0.6649 loss_val: 0.7083 acc_val: 0.5000 time: 0.2617s\n",
            "tensor(0.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0082 loss_train: 0.6068 acc_train: 0.6649 loss_val: 0.7069 acc_val: 0.5000 time: 0.2729s\n",
            "tensor(0.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0083 loss_train: 0.6047 acc_train: 0.6649 loss_val: 0.7056 acc_val: 0.5000 time: 0.2674s\n",
            "tensor(0.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0084 loss_train: 0.6045 acc_train: 0.6649 loss_val: 0.7042 acc_val: 0.5000 time: 0.2595s\n",
            "tensor(0.7042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0085 loss_train: 0.6068 acc_train: 0.6649 loss_val: 0.7030 acc_val: 0.5000 time: 0.2772s\n",
            "tensor(0.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0086 loss_train: 0.6037 acc_train: 0.6649 loss_val: 0.7018 acc_val: 0.5000 time: 0.2662s\n",
            "tensor(0.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0087 loss_train: 0.6090 acc_train: 0.6649 loss_val: 0.7008 acc_val: 0.5000 time: 0.2686s\n",
            "tensor(0.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0088 loss_train: 0.6070 acc_train: 0.6649 loss_val: 0.6998 acc_val: 0.5000 time: 0.2570s\n",
            "tensor(0.6998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0089 loss_train: 0.6054 acc_train: 0.6649 loss_val: 0.6989 acc_val: 0.5000 time: 0.2742s\n",
            "tensor(0.6989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Cannot hit mean loss, will continue\n",
            "Epoch: 0090 loss_train: 0.6052 acc_train: 0.6649 loss_val: 0.6981 acc_val: 0.5000 time: 0.2616s\n",
            "tensor(0.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0091 loss_train: 0.6043 acc_train: 0.6649 loss_val: 0.6974 acc_val: 0.5000 time: 0.2685s\n",
            "tensor(0.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0092 loss_train: 0.6047 acc_train: 0.6649 loss_val: 0.6966 acc_val: 0.5000 time: 0.2674s\n",
            "tensor(0.6966, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0093 loss_train: 0.6046 acc_train: 0.6649 loss_val: 0.6959 acc_val: 0.5000 time: 0.3755s\n",
            "tensor(0.6959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0094 loss_train: 0.6053 acc_train: 0.6649 loss_val: 0.6953 acc_val: 0.5000 time: 0.3716s\n",
            "tensor(0.6953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0095 loss_train: 0.6070 acc_train: 0.6649 loss_val: 0.6947 acc_val: 0.5000 time: 0.3486s\n",
            "tensor(0.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0096 loss_train: 0.6001 acc_train: 0.6649 loss_val: 0.6941 acc_val: 0.5000 time: 0.3560s\n",
            "tensor(0.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0097 loss_train: 0.6012 acc_train: 0.6649 loss_val: 0.6937 acc_val: 0.5000 time: 0.4166s\n",
            "tensor(0.6937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0098 loss_train: 0.6027 acc_train: 0.6649 loss_val: 0.6932 acc_val: 0.5000 time: 0.4118s\n",
            "tensor(0.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0099 loss_train: 0.5994 acc_train: 0.6649 loss_val: 0.6927 acc_val: 0.5000 time: 0.3948s\n",
            "tensor(0.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Cannot hit mean loss, will continue\n",
            "Epoch: 0100 loss_train: 0.6024 acc_train: 0.6649 loss_val: 0.6923 acc_val: 0.5000 time: 0.3385s\n",
            "tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0101 loss_train: 0.6057 acc_train: 0.6649 loss_val: 0.6918 acc_val: 0.5000 time: 0.2655s\n",
            "tensor(0.6918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0102 loss_train: 0.6049 acc_train: 0.6649 loss_val: 0.6913 acc_val: 0.5000 time: 0.2548s\n",
            "tensor(0.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0103 loss_train: 0.6017 acc_train: 0.6649 loss_val: 0.6908 acc_val: 0.5000 time: 0.2715s\n",
            "tensor(0.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0104 loss_train: 0.6011 acc_train: 0.6649 loss_val: 0.6903 acc_val: 0.5000 time: 0.2645s\n",
            "tensor(0.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0105 loss_train: 0.6019 acc_train: 0.6649 loss_val: 0.6897 acc_val: 0.5000 time: 0.2567s\n",
            "tensor(0.6897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0106 loss_train: 0.6000 acc_train: 0.6649 loss_val: 0.6889 acc_val: 0.5000 time: 0.2485s\n",
            "tensor(0.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0107 loss_train: 0.6023 acc_train: 0.6649 loss_val: 0.6882 acc_val: 0.5000 time: 0.2709s\n",
            "tensor(0.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0108 loss_train: 0.5973 acc_train: 0.6649 loss_val: 0.6872 acc_val: 0.5000 time: 0.2721s\n",
            "tensor(0.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0109 loss_train: 0.5997 acc_train: 0.6649 loss_val: 0.6862 acc_val: 0.5000 time: 0.2635s\n",
            "tensor(0.6862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0110 loss_train: 0.5994 acc_train: 0.6649 loss_val: 0.6852 acc_val: 0.5000 time: 0.2760s\n",
            "tensor(0.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0111 loss_train: 0.5984 acc_train: 0.6649 loss_val: 0.6842 acc_val: 0.5000 time: 0.2684s\n",
            "tensor(0.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0112 loss_train: 0.5994 acc_train: 0.6649 loss_val: 0.6832 acc_val: 0.5000 time: 0.2792s\n",
            "tensor(0.6832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0113 loss_train: 0.5976 acc_train: 0.6649 loss_val: 0.6821 acc_val: 0.5000 time: 0.2563s\n",
            "tensor(0.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0114 loss_train: 0.5968 acc_train: 0.6649 loss_val: 0.6810 acc_val: 0.5000 time: 0.2806s\n",
            "tensor(0.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0115 loss_train: 0.5961 acc_train: 0.6649 loss_val: 0.6799 acc_val: 0.5000 time: 0.2643s\n",
            "tensor(0.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0116 loss_train: 0.5928 acc_train: 0.6649 loss_val: 0.6789 acc_val: 0.5000 time: 0.2702s\n",
            "tensor(0.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0117 loss_train: 0.5976 acc_train: 0.6649 loss_val: 0.6778 acc_val: 0.5000 time: 0.2602s\n",
            "tensor(0.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0118 loss_train: 0.5981 acc_train: 0.6649 loss_val: 0.6768 acc_val: 0.5000 time: 0.2775s\n",
            "tensor(0.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0119 loss_train: 0.5909 acc_train: 0.6649 loss_val: 0.6758 acc_val: 0.5000 time: 0.2939s\n",
            "tensor(0.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0120 loss_train: 0.5958 acc_train: 0.6649 loss_val: 0.6747 acc_val: 0.5000 time: 0.2636s\n",
            "tensor(0.6747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0121 loss_train: 0.5909 acc_train: 0.6649 loss_val: 0.6737 acc_val: 0.5000 time: 0.2662s\n",
            "tensor(0.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0122 loss_train: 0.5907 acc_train: 0.6649 loss_val: 0.6727 acc_val: 0.5000 time: 0.2703s\n",
            "tensor(0.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0123 loss_train: 0.5924 acc_train: 0.6649 loss_val: 0.6715 acc_val: 0.5000 time: 0.2678s\n",
            "tensor(0.6715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0124 loss_train: 0.5928 acc_train: 0.6649 loss_val: 0.6704 acc_val: 0.5000 time: 0.2558s\n",
            "tensor(0.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0125 loss_train: 0.5931 acc_train: 0.6649 loss_val: 0.6691 acc_val: 0.5000 time: 0.2735s\n",
            "tensor(0.6691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0126 loss_train: 0.5946 acc_train: 0.6649 loss_val: 0.6678 acc_val: 0.5000 time: 0.2824s\n",
            "tensor(0.6678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0127 loss_train: 0.5882 acc_train: 0.6649 loss_val: 0.6664 acc_val: 0.5000 time: 0.2723s\n",
            "tensor(0.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0128 loss_train: 0.5904 acc_train: 0.6649 loss_val: 0.6649 acc_val: 0.5000 time: 0.2559s\n",
            "tensor(0.6649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0129 loss_train: 0.5914 acc_train: 0.6649 loss_val: 0.6632 acc_val: 0.5000 time: 0.2759s\n",
            "tensor(0.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0130 loss_train: 0.5873 acc_train: 0.6649 loss_val: 0.6615 acc_val: 0.5000 time: 0.2693s\n",
            "tensor(0.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0131 loss_train: 0.5874 acc_train: 0.6649 loss_val: 0.6597 acc_val: 0.5000 time: 0.2572s\n",
            "tensor(0.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0132 loss_train: 0.5851 acc_train: 0.6649 loss_val: 0.6581 acc_val: 0.5000 time: 0.2600s\n",
            "tensor(0.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0133 loss_train: 0.5874 acc_train: 0.6649 loss_val: 0.6564 acc_val: 0.5000 time: 0.2742s\n",
            "tensor(0.6564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0134 loss_train: 0.5861 acc_train: 0.6649 loss_val: 0.6548 acc_val: 0.5000 time: 0.2720s\n",
            "tensor(0.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0135 loss_train: 0.5892 acc_train: 0.6649 loss_val: 0.6533 acc_val: 0.5000 time: 0.2595s\n",
            "tensor(0.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0136 loss_train: 0.5841 acc_train: 0.6649 loss_val: 0.6518 acc_val: 0.5000 time: 0.2612s\n",
            "tensor(0.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0137 loss_train: 0.5829 acc_train: 0.6649 loss_val: 0.6501 acc_val: 0.5000 time: 0.3994s\n",
            "tensor(0.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0138 loss_train: 0.5821 acc_train: 0.6649 loss_val: 0.6484 acc_val: 0.5000 time: 0.3461s\n",
            "tensor(0.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0139 loss_train: 0.5818 acc_train: 0.6649 loss_val: 0.6464 acc_val: 0.5000 time: 0.3463s\n",
            "tensor(0.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0140 loss_train: 0.5821 acc_train: 0.6649 loss_val: 0.6442 acc_val: 0.5000 time: 0.3699s\n",
            "tensor(0.6442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0141 loss_train: 0.5793 acc_train: 0.6649 loss_val: 0.6418 acc_val: 0.5000 time: 0.4071s\n",
            "tensor(0.6418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0142 loss_train: 0.5764 acc_train: 0.6649 loss_val: 0.6395 acc_val: 0.5000 time: 0.4227s\n",
            "tensor(0.6395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0143 loss_train: 0.5786 acc_train: 0.6649 loss_val: 0.6371 acc_val: 0.5000 time: 0.3974s\n",
            "tensor(0.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0144 loss_train: 0.5752 acc_train: 0.6649 loss_val: 0.6348 acc_val: 0.5000 time: 0.2547s\n",
            "tensor(0.6348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0145 loss_train: 0.5761 acc_train: 0.6649 loss_val: 0.6325 acc_val: 0.5000 time: 0.2525s\n",
            "tensor(0.6325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0146 loss_train: 0.5753 acc_train: 0.6649 loss_val: 0.6302 acc_val: 0.5000 time: 0.2780s\n",
            "tensor(0.6302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0147 loss_train: 0.5713 acc_train: 0.6649 loss_val: 0.6281 acc_val: 0.5000 time: 0.2571s\n",
            "tensor(0.6281, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0148 loss_train: 0.5702 acc_train: 0.6649 loss_val: 0.6259 acc_val: 0.5000 time: 0.2591s\n",
            "tensor(0.6259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0149 loss_train: 0.5743 acc_train: 0.6649 loss_val: 0.6236 acc_val: 0.5000 time: 0.2636s\n",
            "tensor(0.6236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0150 loss_train: 0.5750 acc_train: 0.6649 loss_val: 0.6211 acc_val: 0.5000 time: 0.2797s\n",
            "tensor(0.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0151 loss_train: 0.5703 acc_train: 0.6649 loss_val: 0.6186 acc_val: 0.5000 time: 0.2565s\n",
            "tensor(0.6186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0152 loss_train: 0.5689 acc_train: 0.6649 loss_val: 0.6162 acc_val: 0.5000 time: 0.2597s\n",
            "tensor(0.6162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0153 loss_train: 0.5686 acc_train: 0.6649 loss_val: 0.6136 acc_val: 0.5000 time: 0.2805s\n",
            "tensor(0.6136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0154 loss_train: 0.5671 acc_train: 0.6649 loss_val: 0.6108 acc_val: 0.5000 time: 0.2723s\n",
            "tensor(0.6108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0155 loss_train: 0.5638 acc_train: 0.6649 loss_val: 0.6080 acc_val: 0.5000 time: 0.2584s\n",
            "tensor(0.6080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0156 loss_train: 0.5624 acc_train: 0.6649 loss_val: 0.6051 acc_val: 0.5000 time: 0.2654s\n",
            "tensor(0.6051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0157 loss_train: 0.5603 acc_train: 0.6649 loss_val: 0.6022 acc_val: 0.5000 time: 0.2761s\n",
            "tensor(0.6022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0158 loss_train: 0.5584 acc_train: 0.6649 loss_val: 0.5996 acc_val: 0.5000 time: 0.2589s\n",
            "tensor(0.5996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0159 loss_train: 0.5605 acc_train: 0.6649 loss_val: 0.5972 acc_val: 0.5000 time: 0.2632s\n",
            "tensor(0.5972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0160 loss_train: 0.5605 acc_train: 0.6649 loss_val: 0.5948 acc_val: 0.5000 time: 0.2631s\n",
            "tensor(0.5948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0161 loss_train: 0.5576 acc_train: 0.6649 loss_val: 0.5924 acc_val: 0.5000 time: 0.2802s\n",
            "tensor(0.5924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0162 loss_train: 0.5558 acc_train: 0.6702 loss_val: 0.5899 acc_val: 0.5000 time: 0.2646s\n",
            "tensor(0.5899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0163 loss_train: 0.5563 acc_train: 0.6702 loss_val: 0.5873 acc_val: 0.6429 time: 0.2678s\n",
            "tensor(0.5873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0164 loss_train: 0.5532 acc_train: 0.6915 loss_val: 0.5847 acc_val: 0.6786 time: 0.2686s\n",
            "tensor(0.5847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0165 loss_train: 0.5550 acc_train: 0.7021 loss_val: 0.5822 acc_val: 0.6786 time: 0.2683s\n",
            "tensor(0.5822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0166 loss_train: 0.5532 acc_train: 0.7181 loss_val: 0.5800 acc_val: 0.7500 time: 0.2524s\n",
            "tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0167 loss_train: 0.5529 acc_train: 0.7500 loss_val: 0.5780 acc_val: 0.7500 time: 0.2631s\n",
            "tensor(0.5780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0168 loss_train: 0.5507 acc_train: 0.7394 loss_val: 0.5761 acc_val: 0.7500 time: 0.2662s\n",
            "tensor(0.5761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0169 loss_train: 0.5519 acc_train: 0.7500 loss_val: 0.5744 acc_val: 0.7500 time: 0.2762s\n",
            "tensor(0.5744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0170 loss_train: 0.5471 acc_train: 0.7447 loss_val: 0.5726 acc_val: 0.7500 time: 0.2589s\n",
            "tensor(0.5726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0171 loss_train: 0.5527 acc_train: 0.7394 loss_val: 0.5710 acc_val: 0.7500 time: 0.2535s\n",
            "tensor(0.5710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0172 loss_train: 0.5483 acc_train: 0.7340 loss_val: 0.5694 acc_val: 0.7857 time: 0.2827s\n",
            "tensor(0.5694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0173 loss_train: 0.5496 acc_train: 0.7394 loss_val: 0.5677 acc_val: 0.7857 time: 0.2625s\n",
            "tensor(0.5677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0174 loss_train: 0.5452 acc_train: 0.7500 loss_val: 0.5661 acc_val: 0.7857 time: 0.2582s\n",
            "tensor(0.5661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0175 loss_train: 0.5471 acc_train: 0.7394 loss_val: 0.5644 acc_val: 0.7857 time: 0.2629s\n",
            "tensor(0.5644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0176 loss_train: 0.5447 acc_train: 0.7553 loss_val: 0.5629 acc_val: 0.7857 time: 0.2760s\n",
            "tensor(0.5629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0177 loss_train: 0.5442 acc_train: 0.7553 loss_val: 0.5614 acc_val: 0.7500 time: 0.2553s\n",
            "tensor(0.5614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0178 loss_train: 0.5452 acc_train: 0.7606 loss_val: 0.5599 acc_val: 0.7857 time: 0.2562s\n",
            "tensor(0.5599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0179 loss_train: 0.5432 acc_train: 0.7606 loss_val: 0.5584 acc_val: 0.7857 time: 0.2741s\n",
            "tensor(0.5584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0180 loss_train: 0.5422 acc_train: 0.7713 loss_val: 0.5570 acc_val: 0.7857 time: 0.2685s\n",
            "tensor(0.5570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0181 loss_train: 0.5409 acc_train: 0.7872 loss_val: 0.5557 acc_val: 0.7857 time: 0.3822s\n",
            "tensor(0.5557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0182 loss_train: 0.5375 acc_train: 0.7926 loss_val: 0.5548 acc_val: 0.7857 time: 0.3479s\n",
            "tensor(0.5548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0183 loss_train: 0.5422 acc_train: 0.7660 loss_val: 0.5536 acc_val: 0.7857 time: 0.3481s\n",
            "tensor(0.5536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0184 loss_train: 0.5428 acc_train: 0.7660 loss_val: 0.5519 acc_val: 0.7857 time: 0.3689s\n",
            "tensor(0.5519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0185 loss_train: 0.5378 acc_train: 0.7766 loss_val: 0.5501 acc_val: 0.8214 time: 0.4184s\n",
            "tensor(0.5501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0186 loss_train: 0.5400 acc_train: 0.7872 loss_val: 0.5490 acc_val: 0.8214 time: 0.3912s\n",
            "tensor(0.5490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0187 loss_train: 0.5362 acc_train: 0.7872 loss_val: 0.5482 acc_val: 0.8571 time: 0.3888s\n",
            "tensor(0.5482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0188 loss_train: 0.5364 acc_train: 0.7979 loss_val: 0.5471 acc_val: 0.8571 time: 0.2630s\n",
            "tensor(0.5471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0189 loss_train: 0.5337 acc_train: 0.8085 loss_val: 0.5458 acc_val: 0.8571 time: 0.2833s\n",
            "tensor(0.5458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0190 loss_train: 0.5371 acc_train: 0.7872 loss_val: 0.5448 acc_val: 0.8571 time: 0.2631s\n",
            "tensor(0.5448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0191 loss_train: 0.5359 acc_train: 0.8138 loss_val: 0.5439 acc_val: 0.8214 time: 0.2653s\n",
            "tensor(0.5439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0192 loss_train: 0.5336 acc_train: 0.7926 loss_val: 0.5429 acc_val: 0.8214 time: 0.2654s\n",
            "tensor(0.5429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0193 loss_train: 0.5354 acc_train: 0.7979 loss_val: 0.5420 acc_val: 0.8571 time: 0.2713s\n",
            "tensor(0.5420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0194 loss_train: 0.5342 acc_train: 0.7926 loss_val: 0.5412 acc_val: 0.8571 time: 0.2565s\n",
            "tensor(0.5412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0195 loss_train: 0.5316 acc_train: 0.8138 loss_val: 0.5401 acc_val: 0.8571 time: 0.2672s\n",
            "tensor(0.5401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0196 loss_train: 0.5318 acc_train: 0.7819 loss_val: 0.5391 acc_val: 0.9286 time: 0.2626s\n",
            "tensor(0.5391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0197 loss_train: 0.5332 acc_train: 0.7979 loss_val: 0.5383 acc_val: 0.9286 time: 0.2553s\n",
            "tensor(0.5383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0198 loss_train: 0.5296 acc_train: 0.8085 loss_val: 0.5374 acc_val: 0.9286 time: 0.2582s\n",
            "tensor(0.5374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0199 loss_train: 0.5316 acc_train: 0.8032 loss_val: 0.5364 acc_val: 0.9286 time: 0.2779s\n",
            "tensor(0.5364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0200 loss_train: 0.5337 acc_train: 0.8085 loss_val: 0.5354 acc_val: 0.9286 time: 0.2690s\n",
            "tensor(0.5354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0201 loss_train: 0.5323 acc_train: 0.8085 loss_val: 0.5347 acc_val: 0.9286 time: 0.2567s\n",
            "tensor(0.5347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0202 loss_train: 0.5282 acc_train: 0.8032 loss_val: 0.5341 acc_val: 0.9286 time: 0.2764s\n",
            "tensor(0.5341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0203 loss_train: 0.5302 acc_train: 0.8085 loss_val: 0.5335 acc_val: 0.9286 time: 0.2632s\n",
            "tensor(0.5335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0204 loss_train: 0.5251 acc_train: 0.8085 loss_val: 0.5323 acc_val: 0.9286 time: 0.2682s\n",
            "tensor(0.5323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0205 loss_train: 0.5259 acc_train: 0.8085 loss_val: 0.5315 acc_val: 0.9286 time: 0.2634s\n",
            "tensor(0.5315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0206 loss_train: 0.5260 acc_train: 0.8138 loss_val: 0.5309 acc_val: 0.9286 time: 0.2653s\n",
            "tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0207 loss_train: 0.5298 acc_train: 0.8032 loss_val: 0.5308 acc_val: 0.9286 time: 0.2588s\n",
            "tensor(0.5308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0208 loss_train: 0.5260 acc_train: 0.8032 loss_val: 0.5312 acc_val: 0.9286 time: 0.2713s\n",
            "tensor(0.5312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0209 loss_train: 0.5226 acc_train: 0.8138 loss_val: 0.5307 acc_val: 0.9286 time: 0.2653s\n",
            "tensor(0.5307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0210 loss_train: 0.5264 acc_train: 0.8138 loss_val: 0.5291 acc_val: 0.9286 time: 0.2649s\n",
            "tensor(0.5291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0211 loss_train: 0.5222 acc_train: 0.8138 loss_val: 0.5277 acc_val: 0.9286 time: 0.2590s\n",
            "tensor(0.5277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0212 loss_train: 0.5264 acc_train: 0.8085 loss_val: 0.5259 acc_val: 0.9286 time: 0.2750s\n",
            "tensor(0.5259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0213 loss_train: 0.5231 acc_train: 0.8138 loss_val: 0.5250 acc_val: 0.9286 time: 0.2678s\n",
            "tensor(0.5250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0214 loss_train: 0.5214 acc_train: 0.8085 loss_val: 0.5241 acc_val: 0.9286 time: 0.2634s\n",
            "tensor(0.5241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0215 loss_train: 0.5220 acc_train: 0.8138 loss_val: 0.5236 acc_val: 0.9286 time: 0.2656s\n",
            "tensor(0.5236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0216 loss_train: 0.5207 acc_train: 0.8138 loss_val: 0.5238 acc_val: 0.9286 time: 0.2862s\n",
            "tensor(0.5238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0217 loss_train: 0.5196 acc_train: 0.8191 loss_val: 0.5234 acc_val: 0.9286 time: 0.2674s\n",
            "tensor(0.5234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0218 loss_train: 0.5225 acc_train: 0.8138 loss_val: 0.5226 acc_val: 0.9286 time: 0.2647s\n",
            "tensor(0.5226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0219 loss_train: 0.5199 acc_train: 0.8138 loss_val: 0.5207 acc_val: 0.9286 time: 0.2734s\n",
            "tensor(0.5207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0220 loss_train: 0.5163 acc_train: 0.8138 loss_val: 0.5184 acc_val: 0.9286 time: 0.2680s\n",
            "tensor(0.5184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0221 loss_train: 0.5196 acc_train: 0.8191 loss_val: 0.5165 acc_val: 0.9286 time: 0.2698s\n",
            "tensor(0.5165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0222 loss_train: 0.5125 acc_train: 0.8138 loss_val: 0.5152 acc_val: 0.9286 time: 0.2523s\n",
            "tensor(0.5152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0223 loss_train: 0.5128 acc_train: 0.8085 loss_val: 0.5140 acc_val: 0.9286 time: 0.2707s\n",
            "tensor(0.5140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0224 loss_train: 0.5147 acc_train: 0.8085 loss_val: 0.5133 acc_val: 0.9286 time: 0.2987s\n",
            "tensor(0.5133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0225 loss_train: 0.5191 acc_train: 0.8191 loss_val: 0.5126 acc_val: 0.9286 time: 0.3790s\n",
            "tensor(0.5126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0226 loss_train: 0.5119 acc_train: 0.8245 loss_val: 0.5118 acc_val: 0.9286 time: 0.3745s\n",
            "tensor(0.5118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0227 loss_train: 0.5116 acc_train: 0.8245 loss_val: 0.5103 acc_val: 0.9286 time: 0.3278s\n",
            "tensor(0.5103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0228 loss_train: 0.5136 acc_train: 0.8245 loss_val: 0.5084 acc_val: 0.9286 time: 0.3718s\n",
            "tensor(0.5084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0229 loss_train: 0.5062 acc_train: 0.8191 loss_val: 0.5067 acc_val: 0.9286 time: 0.4533s\n",
            "tensor(0.5067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0230 loss_train: 0.5108 acc_train: 0.8191 loss_val: 0.5050 acc_val: 0.9286 time: 0.3852s\n",
            "tensor(0.5050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0231 loss_train: 0.5083 acc_train: 0.8245 loss_val: 0.5033 acc_val: 0.9286 time: 0.4116s\n",
            "tensor(0.5033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0232 loss_train: 0.5076 acc_train: 0.8191 loss_val: 0.5017 acc_val: 0.9286 time: 0.2812s\n",
            "tensor(0.5017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0233 loss_train: 0.5060 acc_train: 0.8298 loss_val: 0.5000 acc_val: 0.9286 time: 0.2715s\n",
            "tensor(0.5000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0234 loss_train: 0.5052 acc_train: 0.8245 loss_val: 0.4986 acc_val: 0.9286 time: 0.2590s\n",
            "tensor(0.4986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0235 loss_train: 0.5040 acc_train: 0.8191 loss_val: 0.4975 acc_val: 0.9286 time: 0.2702s\n",
            "tensor(0.4975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0236 loss_train: 0.5026 acc_train: 0.8298 loss_val: 0.4960 acc_val: 0.9286 time: 0.2715s\n",
            "tensor(0.4960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0237 loss_train: 0.4992 acc_train: 0.8298 loss_val: 0.4948 acc_val: 0.9286 time: 0.2597s\n",
            "tensor(0.4948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0238 loss_train: 0.5015 acc_train: 0.8351 loss_val: 0.4934 acc_val: 0.9286 time: 0.2686s\n",
            "tensor(0.4934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0239 loss_train: 0.5026 acc_train: 0.8351 loss_val: 0.4906 acc_val: 0.9286 time: 0.2799s\n",
            "tensor(0.4906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0240 loss_train: 0.5035 acc_train: 0.8457 loss_val: 0.4873 acc_val: 0.9286 time: 0.2691s\n",
            "tensor(0.4873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0241 loss_train: 0.4959 acc_train: 0.8351 loss_val: 0.4854 acc_val: 0.9286 time: 0.2569s\n",
            "tensor(0.4854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0242 loss_train: 0.5003 acc_train: 0.8298 loss_val: 0.4836 acc_val: 0.9286 time: 0.2605s\n",
            "tensor(0.4836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0243 loss_train: 0.5032 acc_train: 0.8138 loss_val: 0.4827 acc_val: 0.9286 time: 0.2792s\n",
            "tensor(0.4827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0244 loss_train: 0.4962 acc_train: 0.8457 loss_val: 0.4819 acc_val: 0.9286 time: 0.2580s\n",
            "tensor(0.4819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0245 loss_train: 0.4945 acc_train: 0.8404 loss_val: 0.4803 acc_val: 0.9286 time: 0.2634s\n",
            "tensor(0.4803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0246 loss_train: 0.4922 acc_train: 0.8404 loss_val: 0.4782 acc_val: 0.9286 time: 0.2679s\n",
            "tensor(0.4782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0247 loss_train: 0.4976 acc_train: 0.8457 loss_val: 0.4755 acc_val: 0.9286 time: 0.2830s\n",
            "tensor(0.4755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0248 loss_train: 0.4931 acc_train: 0.8298 loss_val: 0.4733 acc_val: 0.9286 time: 0.2566s\n",
            "tensor(0.4733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0249 loss_train: 0.4925 acc_train: 0.8404 loss_val: 0.4718 acc_val: 0.9286 time: 0.2619s\n",
            "tensor(0.4718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0250 loss_train: 0.4881 acc_train: 0.8511 loss_val: 0.4704 acc_val: 0.9286 time: 0.2706s\n",
            "tensor(0.4704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0251 loss_train: 0.4837 acc_train: 0.8511 loss_val: 0.4696 acc_val: 0.9643 time: 0.2662s\n",
            "tensor(0.4696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0252 loss_train: 0.4915 acc_train: 0.8351 loss_val: 0.4682 acc_val: 0.9643 time: 0.2524s\n",
            "tensor(0.4682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0253 loss_train: 0.4875 acc_train: 0.8457 loss_val: 0.4671 acc_val: 0.9286 time: 0.2632s\n",
            "tensor(0.4671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0254 loss_train: 0.4868 acc_train: 0.8457 loss_val: 0.4655 acc_val: 0.9286 time: 0.2713s\n",
            "tensor(0.4655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0255 loss_train: 0.4873 acc_train: 0.8511 loss_val: 0.4630 acc_val: 0.9286 time: 0.2643s\n",
            "tensor(0.4630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0256 loss_train: 0.4777 acc_train: 0.8564 loss_val: 0.4607 acc_val: 0.9643 time: 0.2703s\n",
            "tensor(0.4607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0257 loss_train: 0.4835 acc_train: 0.8404 loss_val: 0.4583 acc_val: 0.9643 time: 0.2675s\n",
            "tensor(0.4583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0258 loss_train: 0.4856 acc_train: 0.8404 loss_val: 0.4570 acc_val: 0.9286 time: 0.2764s\n",
            "tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0259 loss_train: 0.4897 acc_train: 0.8404 loss_val: 0.4549 acc_val: 0.9643 time: 0.2647s\n",
            "tensor(0.4549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0260 loss_train: 0.4874 acc_train: 0.8617 loss_val: 0.4528 acc_val: 0.9643 time: 0.2626s\n",
            "tensor(0.4528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0261 loss_train: 0.4797 acc_train: 0.8404 loss_val: 0.4511 acc_val: 0.9643 time: 0.2611s\n",
            "tensor(0.4511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0262 loss_train: 0.4909 acc_train: 0.8457 loss_val: 0.4497 acc_val: 0.9286 time: 0.2972s\n",
            "tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0263 loss_train: 0.4839 acc_train: 0.8457 loss_val: 0.4484 acc_val: 0.9286 time: 0.2560s\n",
            "tensor(0.4484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0264 loss_train: 0.4743 acc_train: 0.8670 loss_val: 0.4468 acc_val: 0.9286 time: 0.2613s\n",
            "tensor(0.4468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0265 loss_train: 0.4727 acc_train: 0.8564 loss_val: 0.4455 acc_val: 0.9286 time: 0.2618s\n",
            "tensor(0.4455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0266 loss_train: 0.4731 acc_train: 0.8457 loss_val: 0.4440 acc_val: 0.9286 time: 0.2821s\n",
            "tensor(0.4440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0267 loss_train: 0.4867 acc_train: 0.8351 loss_val: 0.4439 acc_val: 0.9286 time: 0.2592s\n",
            "tensor(0.4439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0268 loss_train: 0.4720 acc_train: 0.8564 loss_val: 0.4441 acc_val: 0.9286 time: 0.2604s\n",
            "tensor(0.4441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0269 loss_train: 0.4799 acc_train: 0.8511 loss_val: 0.4411 acc_val: 0.9286 time: 0.4011s\n",
            "tensor(0.4411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0270 loss_train: 0.4889 acc_train: 0.8617 loss_val: 0.4374 acc_val: 0.9286 time: 0.3537s\n",
            "tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0271 loss_train: 0.4749 acc_train: 0.8564 loss_val: 0.4355 acc_val: 0.9643 time: 0.3326s\n",
            "tensor(0.4355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0272 loss_train: 0.4814 acc_train: 0.8351 loss_val: 0.4347 acc_val: 0.9286 time: 0.3786s\n",
            "tensor(0.4347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0273 loss_train: 0.4811 acc_train: 0.8457 loss_val: 0.4351 acc_val: 0.9286 time: 0.4062s\n",
            "tensor(0.4351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0274 loss_train: 0.4721 acc_train: 0.8457 loss_val: 0.4361 acc_val: 0.9286 time: 0.4074s\n",
            "tensor(0.4361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Optimization Finished!\n",
            "Total time elapsed: 79.6933s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRAPH GENERATOR"
      ],
      "metadata": {
        "id": "UcFDzaHOpvxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlikZDjJpDyU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "rollout = 10\n",
        "MAX_NUM_NODES = 28 # for mutag\n",
        "random.seed(200)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, model_path: str, C: list, node_feature_dim: int ,num_class = 2, c=0, hyp1=1, hyp2=2, start=None, nfeat=7, dropout=0.1):\n",
        "        \"\"\"\n",
        "        :param C: Candidate set of nodes (list)\n",
        "        :param start: Starting node (defaults to randomised node)\n",
        "        \"\"\"\n",
        "        super(Generator, self).__init__()\n",
        "        self.nfeat = nfeat\n",
        "        self.dropout = dropout\n",
        "        self.c = c # c为要指定生成类别c的图\n",
        "\n",
        "        self.fc = nn.Linear(nfeat, 8)\n",
        "        self.gc1 = GraphConvolution(8, 16)\n",
        "        self.gc2 = GraphConvolution(16, 24)\n",
        "        self.gc3 = GraphConvolution(24, 32)\n",
        "\n",
        "        # MLP1\n",
        "        # 2 FC layers with hidden dimension 16\n",
        "        self.mlp1 = nn.Sequential(nn.Linear(32, 16), nn.Linear(16, 1))\n",
        "\n",
        "        # MLP2\n",
        "        # 2 FC layers with hidden dimension 24\n",
        "        self.mlp2 = nn.Sequential(nn.Linear(64, 24), nn.Linear(24, 1))\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.hyp1 = hyp1\n",
        "        self.hyp2 = hyp2\n",
        "        self.candidate_set = C\n",
        "\n",
        "        # Default starting node (if any)\n",
        "        if start != None:\n",
        "            self.start = start\n",
        "            self.random_start = False\n",
        "        else:\n",
        "            self.start = random.choice(np.arange(0, len(self.candidate_set)))\n",
        "            self.random_start = True\n",
        "\n",
        "        # Load GCN for calculating reward\n",
        "        self.model = GCN(nfeat=node_feature_dim,\n",
        "                         nclass=num_class,\n",
        "                         dropout=dropout)\n",
        "\n",
        "        self.model.load_state_dict(torch.load(model_path))\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.reset_graph()\n",
        "\n",
        "    def reset_graph(self):\n",
        "        \"\"\"\n",
        "        Reset g.G to default graph with only start node， 生成一个只有1个结点的图\n",
        "        \"\"\"\n",
        "        if self.random_start == True:\n",
        "            self.start = random.choice(np.arange(0, len(self.candidate_set)))\n",
        "\n",
        "        # 初始图除了第1个结点全被mask，这里由于邻接矩阵的边长为MAX_NUM_NODES + len(self.candidate_set)，所以mask的不仅为候选集结点，还有图中的所以虚结点\n",
        "        mask_start = torch.BoolTensor(\n",
        "            [False if i == 0 else True for i in range(MAX_NUM_NODES + len(self.candidate_set))])\n",
        "\n",
        "        adj = torch.zeros((MAX_NUM_NODES + len(self.candidate_set), MAX_NUM_NODES + len(self.candidate_set)),\n",
        "                          dtype=torch.float32)   # 这里adj shape为 [MAX_NUM_NODES + len(self.candidate_set), MAX_NUM_NODES + len(self.candidate_set)] 中间可能有空结点\n",
        "\n",
        "        feat = torch.zeros((MAX_NUM_NODES + len(self.candidate_set), len(self.candidate_set)), dtype=torch.float32)\n",
        "        feat[0, self.start] = 1\n",
        "        feat[np.arange(-len(self.candidate_set), 0), np.arange(0, len(self.candidate_set))] = 1\n",
        "\n",
        "        degrees = torch.zeros(MAX_NUM_NODES)\n",
        "\n",
        "        self.G = {'adj': adj, 'feat': feat, 'degrees': degrees, 'num_nodes': 1, 'mask_start': mask_start}\n",
        "\n",
        "    ## 计算Gt->Gt+1\n",
        "    def forward(self, G_in):\n",
        "        ## G_in为 Gt\n",
        "        G = copy.deepcopy(G_in)\n",
        "\n",
        "        x = G['feat'].detach().clone() # Gt的特征矩阵\n",
        "        adj = G['adj'].detach().clone() # Gt的邻接矩阵\n",
        "\n",
        "        ## 对应 X = GCNs(Gt​,C)\n",
        "        x = F.relu6(self.fc(x))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.relu6(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.relu6(self.gc2(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.relu6(self.gc3(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "        ## pt,start​=Softmax(MLPs(X))\n",
        "        p_start = self.mlp1(x)\n",
        "        p_start = p_start.masked_fill(G['mask_start'].unsqueeze(1), 0)\n",
        "        p_start = F.softmax(p_start, dim=0)\n",
        "        a_start_idx = torch.argmax(p_start.masked_fill(G['mask_start'].unsqueeze(1), -1))\n",
        "\n",
        "        ## pt,end​=Softmax(MLPs([X,x^start​))\n",
        "        # broadcast\n",
        "        x1, x2 = torch.broadcast_tensors(x, x[a_start_idx])\n",
        "        x = torch.cat((x1, x2), 1)  # cat increases dim from 32 to 64\n",
        "\n",
        "        # 计算maskt,end，除了候选集和Gt结点中未被选为初始结点的结点之外，其它均被mask\n",
        "        mask_end = torch.BoolTensor([True for i in range(MAX_NUM_NODES + len(self.candidate_set))])\n",
        "        mask_end[MAX_NUM_NODES:] = False\n",
        "        mask_end[:G['num_nodes']] = False\n",
        "        mask_end[a_start_idx] = True\n",
        "\n",
        "        p_end = self.mlp2(x)\n",
        "        p_end = p_end.masked_fill(mask_end.unsqueeze(1), 0)\n",
        "        p_end = F.softmax(p_end, dim=0)\n",
        "        a_end_idx = torch.argmax(p_end.masked_fill(mask_end.unsqueeze(1), -1))\n",
        "\n",
        "        # Return new G\n",
        "        # If a_end_idx is not masked, node exists in graph, no new node added\n",
        "        if G['mask_start'][a_end_idx] == False:\n",
        "            G['adj'][a_end_idx][a_start_idx] += 1\n",
        "            G['adj'][a_start_idx][a_end_idx] += 1\n",
        "\n",
        "            # Update degrees\n",
        "            G['degrees'][a_start_idx] += 1\n",
        "            G['degrees'][G['num_nodes']] += 1\n",
        "        else:\n",
        "            # Add node\n",
        "            G['feat'][G['num_nodes']] = G['feat'][a_end_idx]\n",
        "            # Add edge\n",
        "            G['adj'][G['num_nodes']][a_start_idx] += 1\n",
        "            G['adj'][a_start_idx][G['num_nodes']] += 1\n",
        "            # Update degrees\n",
        "            G['degrees'][a_start_idx] += 1\n",
        "            G['degrees'][G['num_nodes']] += 1\n",
        "\n",
        "            # Update start mask\n",
        "            G_mask_start_copy = G['mask_start'].detach().clone()\n",
        "            G_mask_start_copy[G['num_nodes']] = False\n",
        "            G['mask_start'] = G_mask_start_copy\n",
        "\n",
        "            G['num_nodes'] += 1\n",
        "\n",
        "        return p_start, a_start_idx, p_end, a_end_idx, G\n",
        "\n",
        "\n",
        "    ### reward函数\n",
        "    def calculate_reward(self, G_t_1):\n",
        "        \"\"\"\n",
        "        Rtr     Calculated from graph rules to encourage generated graphs to be valid\n",
        "                1. Only one edge to be added between any two nodes\n",
        "                2. Generated graph cannot contain more nodes than predefined maximum node number\n",
        "                3. (For chemical) Degree cannot exceed valency\n",
        "                If generated graph violates graph rule, Rtr = -1\n",
        "\n",
        "        Rtf     Feedback from trained model\n",
        "        \"\"\"\n",
        "\n",
        "        rtr = self.check_graph_rules(G_t_1)\n",
        "\n",
        "        rtf = self.calculate_reward_feedback(G_t_1)\n",
        "        rtf_sum = 0\n",
        "        for m in range(rollout):\n",
        "            p_start, a_start, p_end, a_end, G_t_1 = self.forward(G_t_1)\n",
        "            rtf_sum += self.calculate_reward_feedback(G_t_1)\n",
        "        rtf = rtf + rtf_sum * self.hyp1 / rollout\n",
        "\n",
        "        return rtf + self.hyp2 * rtr\n",
        "\n",
        "    def calculate_reward_feedback(self, G_t_1):\n",
        "        \"\"\"\n",
        "        p(f(G_t_1) = c) - 1/l\n",
        "        where l denotes number of possible classes for f\n",
        "        \"\"\"\n",
        "        f = self.model(G_t_1['feat'], G_t_1['adj'])\n",
        "        return f[self.c] - 1 / len(f)\n",
        "\n",
        "\n",
        "    ## graph rules\n",
        "    def check_graph_rules(self, G_t_1):\n",
        "        \"\"\"\n",
        "        For mutag, node degrees cannot exceed valency\n",
        "        \"\"\"\n",
        "        idx = 0\n",
        "\n",
        "        for d in G_t_1['degrees']:\n",
        "            if d != 0:\n",
        "                node_id = torch.argmax(G_t_1['feat'][idx])  # Eg. [0, 1, 0, 0] -> 1\n",
        "                node = self.candidate_set[node_id]  # Eg ['C.4', 'F.2', 'Br.7'][1] = 'F.2'\n",
        "                max_valency = int(node.split('.')[1])  # Eg. C.4 -> ['C', '4'] -> 4\n",
        "\n",
        "                # If any node degree exceeds its valency, return -1\n",
        "                if max_valency < d:\n",
        "                    return -1\n",
        "\n",
        "        return 0\n",
        "\n",
        "\n",
        "    ## 计算loss\n",
        "    def calculate_loss(self, Rt, p_start, a_start, p_end, a_end, G_t_1):\n",
        "        \"\"\"\n",
        "        Calculated from cross entropy loss (Lce) and reward function (Rt)\n",
        "        where loss = -Rt*(Lce_start + Lce_end)\n",
        "        \"\"\"\n",
        "\n",
        "        Lce_start = F.cross_entropy(torch.reshape(p_start, (1, 35)), a_start.unsqueeze(0))\n",
        "        Lce_end = F.cross_entropy(torch.reshape(p_end, (1, 35)), a_end.unsqueeze(0))\n",
        "\n",
        "        return -Rt * (Lce_start + Lce_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CHECK"
      ],
      "metadata": {
        "id": "ClOj0KCGqF7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import torch\n",
        "\n",
        "src_path = '/content/datas/MUTAG_{}'\n",
        "split_train=0.7\n",
        "split_val=0.15\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    nodeidx_features = np.genfromtxt(src_path.format('node_labels.txt'), delimiter=\",\",\n",
        "                                     dtype=np.dtype(int))\n",
        "    node_features = np.zeros((nodeidx_features.shape[0], max(nodeidx_features) + 1))\n",
        "    node_features[np.arange(nodeidx_features.shape[0]), nodeidx_features] = 1\n",
        "\n",
        "\n",
        "    graph_labels = np.genfromtxt(src_path.format('graph_labels.txt'), dtype=np.dtype(int))\n",
        "    graph_labels = encode_onehot(graph_labels)\n",
        "    graph_labels = torch.LongTensor(np.where(graph_labels)[1])\n",
        "\n",
        "\n",
        "    graph_idx = np.genfromtxt(src_path.format('graph_indicator.txt'),dtype=np.dtype(int))\n",
        "    graph_idx = np.array(graph_idx, dtype=np.int32)\n",
        "\n",
        "\n",
        "    edges_unordered = np.genfromtxt(src_path.format('A.txt'), delimiter=\",\",\n",
        "                                    dtype=np.int32)  # (7442,2)\n",
        "\n",
        "    edges_label = np.genfromtxt(src_path.format('edge_labels.txt'), delimiter=\",\",\n",
        "                                dtype=np.int32)\n",
        "\n",
        "    # 邻接矩阵\n",
        "    adj = sp.coo_matrix((edges_label, (edges_unordered[:, 0] - 1, edges_unordered[:, 1] - 1)))\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj) # (3371, 3371)\n",
        "\n",
        "    idx_map = {j: i for i, j in enumerate(graph_idx)} # key, value表示第key个图的起始结点索引号为value\n",
        "    length = len(idx_map.keys())  # 总共有多少个图 , 188\n",
        "    num_nodes = [idx_map[n] - idx_map[n - 1] if n - 1 > 1 else idx_map[n] for n in range(1, length + 1)]  # 一个长度188的list，表示没个图有多少个结点\n",
        "    max_num_nodes = max(num_nodes) # 最大的一个图有多少个结点 实际29\n",
        "    features_list = []\n",
        "    adj_list = []\n",
        "    prev = 0\n",
        "\n",
        "    node_features = normalize(node_features) # (3371, 7)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    adj = adj.todense()\n",
        "\n",
        "    for n in range(1, length + 1):\n",
        "        # entry为图的特征矩阵X\n",
        "        entry = np.zeros((max_num_nodes, max(nodeidx_features) + 1))\n",
        "        entry[:idx_map[n] - prev] = node_features[prev:idx_map[n]]\n",
        "        entry = torch.FloatTensor(entry)\n",
        "        features_list.append(entry)\n",
        "\n",
        "        # entry为图的邻接矩阵A\n",
        "        entry = np.zeros((max_num_nodes, max_num_nodes))\n",
        "        entry[:idx_map[n] - prev, :idx_map[n] - prev] = adj[prev:idx_map[n], prev:idx_map[n]]\n",
        "        entry = torch.FloatTensor(entry)\n",
        "        adj_list.append(entry)\n",
        "\n",
        "        prev = idx_map[n] # prev为下个图起始结点的索引号\n",
        "\n",
        "    num_total = max(graph_idx)\n",
        "    num_train = int(split_train * num_total)\n",
        "    num_val = int((split_train + split_val) * num_total)\n",
        "\n",
        "    idx_train = range(num_train)\n",
        "    idx_val = range(num_train, num_val)\n",
        "    idx_test = range(num_val, num_total)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "\n",
        "    print(graph_labels[idx_train])\n",
        "    print(graph_labels[idx_val])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoWXtD_FqHpZ",
        "outputId": "f4e11552-3c04-4f8f-e9ef-28969ee375e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1])\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRAINING GENERATOR"
      ],
      "metadata": {
        "id": "5eXvbB3Vp-2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.01\n",
        "b1 = 0.9\n",
        "b2 = 0.99\n",
        "hyp1 = 1\n",
        "hyp2 = 2\n",
        "max_gen_step = 10  # T = 10\n",
        "\n",
        "candidate_set = ['C.4', 'N.5', 'O.2', 'F.1', 'I.7', 'Cl.7', 'Br.5']  # C.4表明碳原子的度不超过4\n",
        "model_path = 'model/gcn_first.pth'\n",
        "\n",
        "## 训练generator\n",
        "def train_generator(c=0, max_nodes=5):\n",
        "    g.c = c\n",
        "    for i in range(max_gen_step):\n",
        "        optimizer.zero_grad()\n",
        "        G = copy.deepcopy(g.G)\n",
        "        p_start, a_start, p_end, a_end, G = g.forward(G)\n",
        "\n",
        "        Rt = g.calculate_reward(G)\n",
        "        loss = g.calculate_loss(Rt, p_start, a_start, p_end, a_end, G)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if G['num_nodes'] > max_nodes:\n",
        "            g.reset_graph()\n",
        "        elif Rt > 0:\n",
        "            g.G = G\n",
        "\n",
        "\n",
        "## 生成图\n",
        "def generate_graph(c=0, max_nodes=5):\n",
        "    g.c = c\n",
        "    g.reset_graph()\n",
        "\n",
        "    for i in range(max_gen_step):\n",
        "        G = copy.deepcopy(g.G)\n",
        "        p_start, a_start, p_end, a_end, G = g.forward(G)\n",
        "        Rt = g.calculate_reward(G)\n",
        "\n",
        "        if G['num_nodes'] > max_nodes:\n",
        "            return g.G\n",
        "        elif Rt > 0:\n",
        "            g.G = G\n",
        "\n",
        "    return g.G\n",
        "\n",
        "## 画图\n",
        "def display_graph(G):\n",
        "    G_nx = nx.from_numpy_matrix(np.asmatrix(G['adj'][:G['num_nodes'], :G['num_nodes']].numpy()))\n",
        "    # nx.draw_networkx(G_nx)\n",
        "\n",
        "    layout=nx.spring_layout(G_nx)\n",
        "    nx.draw(G_nx, layout)\n",
        "\n",
        "    coloring=torch.argmax(G['feat'],1)\n",
        "    colors=['b','g','r','c','m','y','k']\n",
        "\n",
        "    for i in range(7):\n",
        "        nx.draw_networkx_nodes(G_nx,pos=layout,nodelist=[x for x in G_nx.nodes() if coloring[x]==i],node_color=colors[i])\n",
        "        nx.draw_networkx_labels(G_nx,pos=layout,labels={x:candidate_set[i].split('.')[0] for x in G_nx.nodes() if coloring[x]==i})\n",
        "    nx.draw_networkx_edges(G_nx,pos=layout,width=list(nx.get_edge_attributes(G_nx,'weight').values()))\n",
        "    nx.draw_networkx_edge_labels(G_nx,pos=layout,edge_labels=nx.get_edge_attributes(G_nx, \"weight\"))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    g = Generator(model_path = model_path, C = candidate_set, node_feature_dim=7 ,c=0, start=0)\n",
        "    optimizer = optim.Adam(g.parameters(), lr=lr, betas=(b1, b2))\n",
        "\n",
        "    for i in range(1, 10):\n",
        "        ## 生成最多分别包括i个结点的图结构\n",
        "        g.reset_graph()\n",
        "        train_generator(c=1, max_nodes=i)\n",
        "        to_display = generate_graph(c=1, max_nodes=i)\n",
        "        display_graph(to_display)\n",
        "        print(g.model(to_display['feat'], to_display['adj']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "n15NwMKbqBak",
        "outputId": "2a0fa405-2636-4ee0-e18e-22d43638a146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-c93076f51218>:56: FutureWarning:\n",
            "\n",
            "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'networkx' has no attribute 'from_numpy_matrix'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6b906d43d4c6>\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mto_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mdisplay_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_display\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_display\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adj'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-6b906d43d4c6>\u001b[0m in \u001b[0;36mdisplay_graph\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m## 画图\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mG_nx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adj'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_nodes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_nodes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;31m# nx.draw_networkx(G_nx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'from_numpy_matrix'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST"
      ],
      "metadata": {
        "id": "iyczD_VhqLWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Load_dataset import load_split_MUTAG_data, accuracy\n",
        "from Model import GCN\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_path = 'model/gcn_first.pth'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    adj_list, features_list, graph_labels, idx_map, idx_train, idx_val, idx_test = load_split_MUTAG_data()\n",
        "    model = GCN(nfeat=features_list[0].shape[1],  # nfeat = 7\n",
        "                nclass=graph_labels.max().item() + 1,  # nclass = 2\n",
        "                dropout=0.1)\n",
        "\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    for i in idx_test:\n",
        "        output = model(features_list[i], adj_list[i])\n",
        "        output = output.unsqueeze(0)\n",
        "        outputs.append(output)\n",
        "    output = torch.cat(outputs, dim=0)\n",
        "\n",
        "    loss_test = F.cross_entropy(output, graph_labels[idx_test])\n",
        "    acc_test = accuracy(output, graph_labels[idx_test])\n",
        "    print(loss_test)\n",
        "    print(acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "6poe4hoUqMWr",
        "outputId": "5cd17399-7b3c-42dd-fe5a-4d1ed5108438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Load_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-11e0ad6d56c0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mLoad_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_split_MUTAG_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGCN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Load_dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}